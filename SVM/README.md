# Метод опорных векторов (SVM - support vector machine)

Метод SVM обладает несколькими замечательными свойствами. Во-первых, обучение SVM сводится к задаче квадратичного программирования, имеющей единственное решение, которое вычисляется достаточно эффективно даже на выборках в сотни тысяч объектов. Во-вторых, решение обладает свойством разреженности: положение оптимальной разделяющей гиперплоскости зависит лишь от небольшой доли обучающих объектов. Они и называются опорными векторами; остальные объекты фактически не задействуются. Наконец, с помощью изящного математическ приёма — введения функции ядра — метод обобщается на случай нелинейных разделяющих поверхностей. Вопрос о выборе ядра, оптимального для данной прикладной задачи, до сих пор остаётся открытой теоретической проблемой.

Предположим, что выборка линейно разделима и существуют значения параметров, при которых функционал числа ошибок равен нулю. Потребуем, чтобы разделяющая гиперплоскость максимально далеко отстояла от ближайших к ней точек обоих классов. Первоначально данный принцип классификации возник из эвристических соображений: вполне естественно полагать, что максимизация зазора (margin) между классами должна способствовать более надёжной классификации.

 Заметим, что параметры линейного порогового классификатора определены с точностью до нормировки: алгоритм a(x) не изменится, если w и w0 одновременно умножить на одну и ту же положительную константу. Удобно выбрать эту константу таким образом, чтобы выполнялось условие
 
![Ну нет ее и все! Отстань!](/SVM/Снимок1.PNG)

Множество точек
![Ну нет ее и все! Отстань!](/SVM/Снимок2.PNG)
описывает полосу, разделяющую классы. Ни один из объектов обучающей выборки не попадает внутрь этой полосы. Границами полосы служат две параллельные гиперплоскости с вектором нормали w. Разделяющая гиперплоскость проходит ровно по середине между ними. Объекты, ближайшие к разделяющей гиперплоскости, лежат на границах полосы, и именно на них достигается минимум. В каждом из классов имеется хотя бы один такой объект, в противном случае разделяющую полосу можно было бы ещё немного расширить и нарушался бы принцип максимального зазора.

![Ну нет ее и все! Отстань!](/SVM/Снимок3.PNG)

Чтобы обобщить постановку задачи на случай линейно неразделимой выборки, позволим алгоритму допускать ошибки на обучающих объектах, но при этом постараемся, чтобы ошибок было поменьше.

Преимущества SVM.

 - Задача квадратичного программирования имеет единственное решение, для нахождения которого разработаны достаточно эффективные методы.
 - Автоматически определяется сложность суперпозиции — число нейронов первого слоя, равное числу опорных векторов.
 - Максимизация зазора между классами улучшает обобщающую способность.
 
Недостатки SVM.

 - Неустойчивость к шуму в исходных данных. Объекты-выбросы являются опорными и существенно влияют на результат обучения.
 - До сих пор не разработаны общие методы подбора ядер под конкретную задачу. На практике «вполне разумные» ядра, построенные с учётом специфики задачи, могут и не обладать свойством положительной определённости.
 - Подбор параметра C требует многократного решения задачи.
